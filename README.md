## 🧠 Q\&A ChatBot

This is a simple Question-Answering ChatBot built using **LangChain**, **Streamlit**, and **OPEN AI & Ollama** models. Ollama takes a user question as input and generates a response using a locally hosted `gemma:2b` LLM. OpenAI will require API_KEY to generate the responses. 

---

### 🚀 Features

* Chat interface using **Streamlit**
* Uses **LangChain** with `ChatOllama` to call local LLM
* Adjustable **temperature** and **max tokens** via sidebar
* Customizable system prompt for assistant behavior

---

### 📦 Requirements

Install dependencies using:

```bash
pip install -r requirements.txt
```

> Ensure you have [Ollama](https://ollama.com/) installed and running locally, with the `gemma:2b` model downloaded.

---

### 🔧 Environment Variables

Create a `.env` file with your **LangSmith** API key:

```
LANGCHAIN_API_KEY=your_langsmith_api_key
```

---

### 🖥️ Running the App

```bash
streamlit run app.py
```

(Assuming your file is named `app.py` — rename accordingly.)

---

### 🧪 Sample Usage

1. Open the Streamlit app in your browser.
2. Enter a question like:

   ```
   What is the capital of France?
   ```
3. Adjust `temperature` and `max_tokens` in the sidebar.
4. Get the answer generated by the `gemma:2b` model via LangChain!

---

### 📁 Project Structure

```
.
├── app.py
├── requirements.txt
├── .env
```

---

### ✅ Example Models

You can switch to other Ollama models by changing:

```python
llm = ChatOllama(model='gemma:2b')
```

to something like:

```python
llm = ChatOllama(model='llama3')
```

---

### 🧠 Note

This app assumes Ollama is **running locally** and has the required model pulled via:

```bash
ollama run gemma:2b
```

