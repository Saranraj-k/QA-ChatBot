## ğŸ§  Q\&A ChatBot

This is a simple Question-Answering ChatBot built using **LangChain**, **Streamlit**, and **OPEN AI & Ollama** models. Ollama takes a user question as input and generates a response using a locally hosted `gemma:2b` LLM. OpenAI will require API_KEY to generate the responses. 

---

### ğŸš€ Features

* Chat interface using **Streamlit**
* Uses **LangChain** with `ChatOllama` to call local LLM
* Adjustable **temperature** and **max tokens** via sidebar
* Customizable system prompt for assistant behavior

---

### ğŸ“¦ Requirements

Install dependencies using:

```bash
pip install -r requirements.txt
```

> Ensure you have [Ollama](https://ollama.com/) installed and running locally, with the `gemma:2b` model downloaded.

---

### ğŸ”§ Environment Variables

Create a `.env` file with your **LangSmith** API key:

```
LANGCHAIN_API_KEY=your_langsmith_api_key
```

---

### ğŸ–¥ï¸ Running the App

```bash
streamlit run app.py
```

(Assuming your file is named `app.py` â€” rename accordingly.)

---

### ğŸ§ª Sample Usage

1. Open the Streamlit app in your browser.
2. Enter a question like:

   ```
   What is the capital of France?
   ```
3. Adjust `temperature` and `max_tokens` in the sidebar.
4. Get the answer generated by the `gemma:2b` model via LangChain!

---

### ğŸ“ Project Structure

```
.
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
```

---

### âœ… Example Models

You can switch to other Ollama models by changing:

```python
llm = ChatOllama(model='gemma:2b')
```

to something like:

```python
llm = ChatOllama(model='llama3')
```

---

### ğŸ§  Note

This app assumes Ollama is **running locally** and has the required model pulled via:

```bash
ollama run gemma:2b
```

